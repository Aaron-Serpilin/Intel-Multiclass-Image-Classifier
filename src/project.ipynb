{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project File\n",
    "Given the magnitude of the RestNet CNN, it is unfeasible to run it on a local CPU. Hence, in order to run it on Google Collab, all the contents will be fused onto this file to run it there. Aside from the file headings, I will not put the rest of the markdowns here. Those can be found on the files themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Define the dataset and paths\n",
    "dataset = 'puneet6060/intel-image-classification'\n",
    "\n",
    "# Navigate one level up to place the data folder at the same level as the src folder\n",
    "base_path = Path(\"..\")  # Represents one level above the current directory\n",
    "data_path = base_path / 'data'\n",
    "zip_path = data_path / 'intel-image-classification.zip'\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the zip file if it doesn't already exist\n",
    "if not zip_path.exists():\n",
    "    print(f\"Downloading {dataset} dataset...\")\n",
    "    subprocess.run(['kaggle', 'datasets', 'download', '-d', dataset, '-p', str(data_path)])\n",
    "else:\n",
    "    print(f\"{zip_path} already exists. Skipping download.\")\n",
    "\n",
    "# Extract the dataset directly into the data folder\n",
    "if not any(data_path.iterdir()):  # Check if data folder is empty\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "else:\n",
    "    print(f\"{data_path} already exists and is not empty. Skipping extraction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = data_path / \"intel-image-classification\"\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = image_path / \"seg_train\" / \"seg_train\"\n",
    "test_dir = image_path / \"seg_test\" / \"seg_test\"\n",
    "pred_dir = image_path / \"seg_pred\"\n",
    "\n",
    "train_dir, test_dir, pred_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "image_path_list = list(image_path.glob(\"*/*/*/*.jpg\")) # This line gets all the images by constantly traversing every single item in the given directory and the following ones as well\n",
    "# print(image_path_list)\n",
    "# print(f\"The Found List Length is {len(image_path_list)}\\n\")\n",
    "\n",
    "# Picking a random image\n",
    "random_image_path = random.choice(image_path_list)\n",
    "# The image class is the name of the directory where the image is stored\n",
    "image_class = random_image_path.parent.stem\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "# Metadata\n",
    "# print(f\"Random image path: {random_image_path}\")\n",
    "# print(f\"Image class: {image_class}\")\n",
    "# print(f\"Image height: {img.height}\")\n",
    "# print(f\"Image width: {img.width}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turn the image into an array\n",
    "img_as_array = np.asarray(img)\n",
    "img_as_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),         # Resize all images to the same dimensions\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(10),         # Rotate images randomly within a range\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Adjust image color properties\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31), # Randomly select an additional data augmentation strategy\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# You normally do not manipulate your test data in terms of data augmentation so we define a separate transform\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
    " \n",
    "  if seed:\n",
    "    random.seed(seed)\n",
    "  random_image_paths = random.sample(image_paths, k=n)\n",
    "  \n",
    "  for image_path in random_image_paths:\n",
    "    with Image.open(image_path) as f:\n",
    "      fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "      ax[0].imshow(f)\n",
    "      ax[0].set_title(f\"Original\\nSize: {f.size}\")\n",
    "      ax[0].axis(False)\n",
    "\n",
    "      # Transform and plot target image\n",
    "      transformed_image = transform(f).permute(1, 2, 0) # note we will need to change shape for matplotlib (C, H, W) -> (H, W, C)\n",
    "      ax[1].imshow(transformed_image)\n",
    "      ax[1].set_title(f\"Transformed\\nShape: {transformed_image.shape}\")\n",
    "      ax[1].axis(\"off\")\n",
    "\n",
    "      fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "\n",
    "# plot_transformed_images(image_paths=image_path_list,\n",
    "#                         transform=train_transform,\n",
    "#                         n=3,\n",
    "#                         seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The relevant paths: train_dir: {train_dir}\\ntest_dir: {test_dir}\\nimage_path: {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                  transform=train_transform,\n",
    "                                  target_transform=None)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                 transform=train_transform)\n",
    "\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.classes, test_data.classes, len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data[0][0], train_data[0][1]\n",
    "print(f\"The img is: {img}\\nThe shape is: {img.shape}\\nThe label is: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data[0][0], train_data[0][1]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with the bottleneck architecture. Key integration of the RestNet50 architecture aimed at tackling the vanishing gradient problem.\n",
    "    Essentially, this block provides the model with a helper path that skips some layers from the input to the output, allowing the residual to be learned more easily. \n",
    "    This situation arises when the gradients become increasingly smaller, causing the earlier layers during back propagation to receive exponentially smaller gradients, preventing the model from learning.\n",
    "    \n",
    "    This block contains three mini-layers: 1x1, 3x3, and 1x1 convolutions. We compress the data, then extract spatial features, and then compress again to its original state. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn_block_1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.conv_block_2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_block_2 = nn.BatchNorm2d(mid_channels)\n",
    "        self.conv_block_3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn_block_3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels: # True if there are mismatched dimensions amongst the input and output channels\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False), # 1x1 convolution\n",
    "                nn.BatchNorm2d(out_channels) # We normalize to match the input to the size of the output\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x) # Apply the shortcut (identity or adjusted input)\n",
    "        x = nn.ReLU()(self.bn_block_1(self.conv_block_1(x))) # 1st layer: 1x1 convolution + batch norm + ReLU\n",
    "        x = nn.ReLU()(self.bn_block_2(self.conv_block_2(x))) # 2nd layer: 3x3 convolution + batch norm + ReLU\n",
    "        x = self.bn_block_3(self.conv_block_3(x)) # 3rd layer: 1x1 convolution + batch norm\n",
    "        x += shortcut # Add shortcut (residual connection)\n",
    "        return nn.ReLU()(x) # Apply ReLU to the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class RestNet(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_shape, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # This corresponds to the 4 residual stages after the initial convolution and pooling layers. Early stages focus on basic patterns while later stages focus on abstract representations\n",
    "        self.conv_block_2 = self._make_stage(64, 64, 256, num_blocks=3, stride=1) # Extracts low-level features like edges and simple textures without spatial reduction.\n",
    "        self.conv_block_3 = self._make_stage(256, 128, 512, num_blocks=4, stride=2) # Captures more complex features and reduces the spatial resolution\n",
    "        self.conv_block_4 = self._make_stage(512, 256, 1024, num_blocks=6, stride=2) # Processes high-level features like object parts or shapes and further reduces spatial resolution\n",
    "        self.conv_block_5 = self._make_stage(1024, 512, 2048, num_blocks=3, stride=2) # Extracts the most abstract and high-level features, preparing for the classification head\n",
    "\n",
    "        # Classifier. It converts the high-level feature maps into class predictions\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, output_shape)\n",
    "        )\n",
    "\n",
    "    # Creates a residual stage by stacking residual blocks. \n",
    "    # The blocks in the stage work hierarchically to extract increasingly complex features while potentially reducing the spatial dimensions of the feature maps.\n",
    "    def _make_stage(self, in_channels, mid_channels, out_channels, num_blocks, stride):\n",
    "\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = [] # to store the residual blocks\n",
    "\n",
    "        for stride in strides:\n",
    "\n",
    "            layers.append(ResidualBlock(in_channels, mid_channels, out_channels, stride))\n",
    "            in_channels = out_channels\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [Batch Size, Color Channels, Height, Width]\n",
    "        # print(f\"The original shape is {x.shape}\")\n",
    "        x = self.conv_block_1(x)\n",
    "        # print(f\"The shape after conv_block_1 is {x.shape}\")\n",
    "        x = self.conv_block_2(x)\n",
    "        # print(f\"The shape after conv_block_2 is {x.shape}\")\n",
    "        x = self.conv_block_3(x)\n",
    "        # print(f\"The shape after conv_block_3 is {x.shape}\")\n",
    "        x = self.conv_block_4(x)\n",
    "        # print(f\"The shape after conv_block_4 is {x.shape}\")\n",
    "        x = self.conv_block_5(x)\n",
    "        # print(f\"The shape after conv_block_5 is {x.shape}\")\n",
    "        x = self.classifier(x)\n",
    "        # print(f\"The final shape is {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = RestNet(input_shape=3, output_shape=1000).to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.unsqueeze(0).shape # [Batch Size, Color Channels, Width, Height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import torchinfo\n",
    "except:\n",
    "  !pip install torchinfo\n",
    "  import torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model_0, input_size=[1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step (model: torch.nn.Module,\n",
    "                data_loader: torch.utils.data.DataLoader,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                accuracy_fn,\n",
    "                device: torch.device = device):\n",
    "    \n",
    "    train_loss, train_accuracy = 0, 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X) # Forward pass, outputs raw logits\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_accuracy += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1)) # transforms from logits to labels\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # We adjust to get these metrics per batch, and not the total\n",
    "    train_loss /= len(data_loader)\n",
    "    train_accuracy /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_accuracy:.2f}%]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    \n",
    "    test_loss, test_accuracy = 0, 0\n",
    "\n",
    "    model.eval()  # Turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)\n",
    "\n",
    "    with torch.inference_mode(): # Turns off gradient tracking and a couple more things behind the scenes\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            # test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            # test_accuracy += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            test_accuracy += accuracy_fn(y_true=y, y_pred=test_pred_logits.argmax(dim=1)) # transforms from logits to labels\n",
    "\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_accuracy = test_accuracy / len(dataloader)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5,\n",
    "          device=device):\n",
    "\n",
    "  results = {\"train_loss\": [],\n",
    "             \"train_acc\": [],\n",
    "             \"test_loss\": [],\n",
    "             \"test_acc\": []}\n",
    "\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = train_step(model=model,\n",
    "                                       data_loader=train_dataloader,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       optimizer=optimizer,\n",
    "                                       accuracy_fn=accuracy_fn,\n",
    "                                       device=device)\n",
    "    test_loss, test_acc = test_step(model=model,\n",
    "                                    data_loader=test_dataloader,\n",
    "                                    loss_fn=loss_fn,\n",
    "                                    accuracy_fn=accuracy_fn,\n",
    "                                    device=device)\n",
    "\n",
    "    print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "    results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr=0.001)\n",
    "\n",
    "start_time = timer()\n",
    "\n",
    "model_0_results = train(model_0,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        device=device)\n",
    "\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
